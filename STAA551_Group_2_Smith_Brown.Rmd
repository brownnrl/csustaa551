---
title: "STAA 551 Case Study - Home Price Regression Model"
output: pdf_document
author: "Nelson Brown and Bryce Smith"
header-includes:
- \usepackage{dcolumn}
- \usepackage{float}
- \usepackage{floatflt}
- \usepackage[singlelinecheck=false]{caption}
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
---
```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, dev='pdf')
library(car)
library(ggplot2)
library(grid)
library(gridExtra)
library(ggthemes)
library(lubridate)
library(GGally)
library(scales)
library(dplyr)
library(stargazer) # Used for latex tables to summarize the data and models
```

# Introduction

This case study focuses on creating a regression model based on a data set of transactions provided for the Seattle area in with properties of homes such as their date built, number of bedrooms, square footage of the living space, and other features that will be described in the next section.  We explore the methods of building this model up from those predictors which provide the strongest correlation with our price response variable, removing those predictors which have collinearity with others included in the model, explore interactions, and examine those models which explain the variation in the response.  We perform a diagnostic and residual analysis on the best performing model to ensure it does not violate basic assumptions of the OLS model, and utilize transformations to modify the model to fit within those assumptions at the cost of a marginal amount of explained variation.  Finally, we examine the results of the final model and describe some of the uses and limitations of the model.

```{r, echo=FALSE, results='hide'}

# SECTION: HEADER AND INTRODUCTION, INITIAL DATA READ

# Helper function for formatting
sci.notation <- function(value) {
  formatC(value, format = "e", digits = 2) 
}

# Read the Data
df <- read.csv('Seattle.csv', strip.white = TRUE, stringsAsFactors = FALSE)
# Clean the Data
df$date <- ymd(substr(df$date,1,nchar(df$date) - 7)) # Convert string to date object
df$was_renovated <- as.factor(with(df, 
     ifelse(yr_renovated > 0, 
            1, 
            0
          )
 ))
df$has_basement <- as.factor(with(df, 
     ifelse(sqft_basement > 0, 
            1, 
            0
          )
 ))
```


# Summary Statistics and Graphics

```{r, echo=FALSE, results='hide'}

# SECTION: SUMMARY STATISTICS AND GRAPHCIS

# Define our quantitative values of interest
df.quant <- df %>% dplyr::select(price,
                          sqft_living,
                          sqft_lot,
                          bedrooms,
                          bathrooms,
                          view,
                          grade,
                          sqft_above,
                          yr_built,
                          date)
```

Our housing data frame had a number of quantitative predictors.  One set are square footage of features of the house such as `sqft_living` or the amount of total space that can be lived in (sans attic, basement, or garage space), `sqft_lot` or square footage of the entire property, `sqft_above` which is living space above the basement level, and `yr_built` or the year the home was built.  The first few rows of these values are displayed in the table below.


```{r, echo=FALSE, results='asis'}

# Print head of initial dataframe
stargazer(head(df.quant %>% dplyr::select(-date)), 
          rownames=FALSE, 
          summary=FALSE, 
          header=FALSE, 
          title="First Four Rows for Quantitative Values on Seattlle Housing Dataframe")
```

Summary statistics for these data points (there exist 613 total rows in the dataset with no missing values) are shown in Table 2.

```{r, echo=FALSE, results='asis'}

# Summarize initial dataframe
stargazer(df.quant,
          header=FALSE,
          omit.summary.stat=c('N'),
          title="Summary Statistics for Values on Seattle Housing Dataframe",
          label="summary-stats")
```

Text text text.



## Quantitative Values

Text text text.

```{r, echo=FALSE, fig.height=6, fig.width=10, fig.align="center", fig.cap="Correlogram of Quantitative or Ordinal Predictors and Response Variable"}

# SECTION: QUANTITATIVE VALUES

# Quantitative Data Pairs
ggpairs(df.quant, progress=FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Text text text.

## Discrete and Categorical Values

Text text text

```{r, echo=FALSE, fig.height=3, fig.width=7, fig.cap="Violin Plots of Categorical Values Interpreted as Factors"}

# SECTION: DISCRETE AND CATEGORICAL VALUES

plot_price_by_cat <- function(df, cat_var, cat_var_name) {
  ggplot(df, aes(x=cat_var, y=price, fill=cat_var)) +
    scale_colour_solarized("red") +
    geom_violin(aes(color=cat_var)) +
    geom_boxplot(width=0.1) +
    xlab(cat_var_name) +
    ylab("Price") +
    scale_y_continuous(labels = scales::dollar_format(scale = .000001, suffix="M")) +
    theme(legend.position="none")
}

q <- plot_price_by_cat(df, as.factor(df$view), "View")
r <- plot_price_by_cat(df, as.factor(df$grade), "Grade")
t <- plot_price_by_cat(df, as.factor(df$waterfront), "Waterfront")
w <- plot_price_by_cat(df, as.factor(df$floors), "Floors")
x <- plot_price_by_cat(df, as.factor(df$condition), "Condition")
y <- plot_price_by_cat(df, as.factor(df$has_basement), "Basement")

grid.arrange(grobs=list(q, r, t, 
                        w, x, y),
             ncol=3,
             top="Price by Categorical Variable") 
```

\newpage


# Analysis

## Bottom-Up Model from Variable Added Last t-test

This analysis began with analyzing a correlogram of the dataset which was presented in figure __.  Since there was a single most correlated predictor `sqft_living` with our response variable `price`, and the second highest predictor `sqft_above` was a directly proportional quantity to `sqft_living` which yielded high colinearity we decided to build a model bottom-up starting with `price` predicted by `sqft_living`.  We would sequentially add predictors to the model while assessing model performance. 

```{r,echo=FALSE, results='hide'}

# SECTION BOTTOM-UP MODEL FROM VARIABLE ADDED LAST t-TEST

lm.price.1 <- lm(formula = price ~ sqft_living, data = df)
lm.price.2 <- lm(formula = price ~ sqft_living + grade, data=df)
```

As seen in model (1) of table 3 on the next page, it was clear that we had strong evidence for a linear association between price and `sqft_living` and that the adjusted $R^2$ amount of explained variation accounted for roughly 52% of the variation in price.  This would remain throughout our analysis the most significant predictor that we could ascertain contributed to the model.  We then proceeded to add `grade` to the model. Adding `grade` to the model proved to be helpful, as we noticed a significant p-value for the variables added last test which was 0.000251. 

```{r,echo=FALSE,results='hide'}
lm.price.3.not.included <- lm(formula = price ~ sqft_living + grade + bathrooms, 
                              data = df)
lm.3.pvalue <- sci.notation(summary(lm.price.3.not.included)$coefficients[3,4])
lm.price.4 <- lm(formula = price ~ sqft_living + grade + view, 
            data = df)

```

Next we added bathrooms into our model and received a p-value of `r lm.3.pvalue` for the variables added last t-test so it was safe to conclude that it was not helpful to our model with `sqft_living` and `grade` already included. However, our next variable we added to our model, `view`, did prove to be helpful with a p-value of 2e-16.

```{r, echo=FALSE,results='hide'}
lm.price.5 <- lm(formula = price ~ 
                   sqft_living + 
                   grade + 
                   view + 
                   yr_built,
            data = df)
```

The model with these three variables determine an adjusted $R^2$ which explained roughly 59% of the variation.  This model is  shown as model (2) in table 4.  We next looked at bathrooms and bedrooms into the model based on the correlation with price.  `bedrooms` was chosen first as it had only a 0.428 correlation with `grade` versus the 0.740 correlation of `bathrooms` with `grade`. It was clear with the results from the summary table that bedrooms hardly explained any more variation in price, and was not helpful in our model so we removed that predictor.  `bathrooms` showed a similar result and was not included in our analysis.

We did want to try the variable `yr_built` in the model since we believed it was important to attempt to add a variable that represented the age of the house. Adding `yr_built` into our model proved to be significant with `sqft_living`, `grade` and `view` already present in the model.  The results of this model are annotated as model (2) in Table 3.

### Adding Categorical Indicator Predictors

```{r, include=FALSE}

# SECTION: ADDING CATEGORICAL INDICATOR PREDICTORS

lm.price.5b <- lm(formula = price ~ 
                   sqft_living + 
                   grade + 
                   view + 
                   yr_built + 
                   waterfront,
                  data = df)
```

At this point in our analysis we concluded to stop adding quantitative and quantitative monotonic ordinal variables, and looked at the categorical variables which could be treated as indicator or dummy variables in our model.  Starting with waterfront, we found that this indicator was helpful in our model and increased our adjusted $R^2$ value to approximately 0.65. 

Adding basement and renovated as indicators to our model, in the same fashion as we added waterfront to the model, ended up not being helpful to our model with `sqft_living`, `grade`, `view`, and `yr_built` already present. We also added these two indicators, separately, with `waterfront` in the model, and these predictors added were not helpful either. We were satisfied with the five predictors: `sqft_living`, `grade`, `view`, `yr_built`, and `waterfront` being used in our model.

### Comparison of Bottom-Up Model with Top-Down Model using ANOVA tables

```{r, include=FALSE}

# SECTION: COMPARISON OF BOTTOM-UP MODEL WITH TOP-DOWN MODEL USING ANOVA

# Top-down model building using ANOVA analysis between a nearly
# full model and our bottom-up analysis, a nearly full model
# and a reduced model, and finally comparative metrics between
# the reduced model and the model built from a bottom-up analysis.


lm.nearfull <- lm(price ~ 
                    bedrooms + 
                    bathrooms + 
                    sqft_living + 
                    sqft_lot + 
                    floors + 
                    waterfront + 
                    view + 
                    condition + 
                    grade + 
                    sqft_above + 
                    yr_built , data=df)
anova.1 <- anova(lm.nearfull, lm.price.5b)
anova.1.p.value <-formatC(anova.1$`Pr(>F)`[2], format = "e", digits = 2) 
lm.reduced <- lm(price ~ bedrooms + 
                   bathrooms + 
                   sqft_living + 
                   waterfront + 
                   view + 
                   grade + 
                   yr_built, data=df)
anova.2 <- anova(lm.nearfull, lm.reduced)
anova.2.p.value <-round(anova.2$`Pr(>F)`[2], 3) 
# collect adjusted R^2
nearfull.r.adjusted <- summary(lm.nearfull)
reduced.r.adjusted <-round(summary(lm.nearfull)$adj.r.squared,3)
lm5.r.adjusted <-round(summary(lm.price.5)$adj.r.squared,3)

```

We wanted to ensure that our model doesn't disagree with an alternative analysis built from a full model in that we did not miss any potential predictors that may add to our model's explained variation while offering strong evidence that they linearly associated with the predictor.  To accomplish this, we first compared a model with nearly every predictor with the exception of those predictors that could be eliminated due to their intrinsic nature or prior inspection. For instance, the id field or date of transaction was not considered in the full model.  An ANOVA was conducted between these models with an F-test statistic value of `r round(anova.1$F[2], 3)` and associated p-value `r anova.1.p.value` indicated that their may be some predictors which may be linearly associated with the response variable.

We then removed the predictors from the nearly full model by examining those with the weakest p-value evidence, and ended up with a reduced model which discarded `floors`, `sqft_above`, and `sqft_lot`.  The reduced model when compared with the full model through an ANOVA table had a F-test statistic of `r round(anova.2$F[2], 3)` and associated p-value of `r anova.2.p.value` which gave fair evidence that the removed predictors were not linearly associated with `price`. Comparing the adjusted $R^2$ value of this reduced model of `r reduced.r.adjusted` with the model we arrived out built bottom-up analysis using variable last added t-tests from initial predictors and it's adjusted $R^2$ of `r lm5.r.adjusted` indicates that the additional explained variation is minimal.  

### Exploring Interactions

Proceeding with model 2 in our analysis, we needed to assess the correlations between our predictors. When doing this, it was clear that we had evidence of colinearity between `sqft_living` and `grade`. With this knowledge we sought to assess whether an interaction between these two variables would be helpful to our model. As seen in model (3) in Table 3, adding this interaction term proved to be helpful in our model and increased the variation in `price` explained by the model to 0.706 as expressed in the adjusted $R^2$.  At this point, we moved on to the diagnostic plots and residuals analysis.  Models (4) and (5) in Table 3 will be explained in the following section.

```{r, echo=FALSE, results='hide'}

# SECTION: EXPLORING INTERACTIONS

lm.price.6 <- lm(formula = price ~ 
                   sqft_living + 
                   grade + 
                   view + 
                   yr_built + 
                   waterfront + 
                   sqft_living*grade, 
                   data = df)

# Corrections and transformations applied based on diagnostic plots
# (performed before diagnostic plots as to provide input to table 4
#  used in the report which we didn't want separated too far from
#  the bottom-up model building section of analysis)

lm.price.7 <- lm(formula = log(price) ~ 
                   log(sqft_living) + 
                   grade +
                   view + 
                   yr_built + 
                   waterfront + 
                   log(sqft_living)*grade, 
                   data = df)
# Final model
lm.price.8 <- lm(formula = 
                   log(price) ~ 
                   log(sqft_living) + 
                   grade + 
                   view + 
                   yr_built, 
                   data = df)
lm.price.final <- lm.price.8
```



\newpage
\blandscape
```{r, results="asis", echo=FALSE}
stargazer(lm.price.1, 
          lm.price.5,
          lm.price.6, 
          lm.price.7, 
          lm.price.8, 
          header=FALSE,
          align=T,
          label="ModelComparison",
          title="Iteratively Built Model Comparison")
```
\elandscape
\newpage


```{r, include=FALSE}

# SECTION: DIAGNOSTIC PLOTS AND RESIDUAL ANALYSIS

# In order to save space, we had to flow some text around these
# diagnostic figures, but I think it worked well to keep the 
# figures near their explanation.

png("diagnostics_prior_to_transform.png", height = 350)
# 2. Create the plot
par(mfrow = c(1,3))
plot(lm.price.6,c(1,2,4))
# 3. Close the file
dev.off()
# Box-cox examining most likely model predictor exponent
png("boxcox.png", height = 350)
boxCox(lm.price.6, main="Box-Cox Plot of model")
dev.off()

png("transformations.png", height = 350)
par(mfrow = c(1,2))
p <- ggplot(df, aes(x=sqft_living)) + geom_density()
q <- ggplot(df, aes(x=log(sqft_living))) + geom_density()
r <- ggplot(df, aes(x=price)) + geom_density()
s <- ggplot(df, aes(x=log(price))) + geom_density()
grid.arrange(grobs=list(p, q,
                        r, s),
             ncol=2,
             top="Transformations on Skewed Distributions") 
dev.off()

```


## Diagnostic Plots and Residuals Analysis

\begin{floatingfigure}[r]{.5\textwidth}  
 \begin{center}
  \includegraphics[width=.5\textwidth]{diagnostics_prior_to_transform.png}  
  \caption{Diagnostic Plots} 
%  \includegraphics[width=.5\textwidth]{transformations.png}  
%  \caption{Transformations of Skewed Distributions} 
%  \includegraphics[width=.5\textwidth]{studentresids.png}  
%  \caption{Externalized Student Residuals} 
\end{center}
\end{floatingfigure}
Satisfied with the model derived up to this point , we proceeded to check the diagnostic plots to check the validity of our assumptions. After review of these plots, it was clear we had problems addressing our assumptions of linearity, mean zero for the errors and constant variance of the errors. This can be shown with the fanning out of the points in the residuals vs fitted plot, and the lack of linearity in the normal QQ plot. We can also see that we have observations with very large Cook’s Distance, which may be negatively affecting our model. Checking the residuals versus the predicted values confirmed what we saw in the previous three plots and we concluded that our assumptions were violated.

To address the violations of our assumptions, we entertained the idea of transforming some of the variables used in the model. We could see from the correlogram that both our response, `price`, and predictor, `sqft_living`, had heavily right skewed distributions. We decided to apply a logarithmic transformation after observing a box-cox plot indicating that the power transformation would be justified. We reapplied these into our model. 

\begin{floatingfigure}[r]{.5\textwidth}  
 \begin{center}
  \includegraphics[width=.5\textwidth]{transformations.png}  
  \caption{Transformations of Skewed Distributions} 
%  \includegraphics[width=.5\textwidth]{studentresids.png}  
%  \caption{Externalized Student Residuals} 
\end{center}
\end{floatingfigure}
Doing so we saw our interaction between the now logarithmic transformation of sqft_living and grade become not helpful to our model. We also, saw that the indicator, waterfront, was no longer helpful to the model. This can be seen in model (4) of Table 3. In an effort to satisfy our assumptions of the model, it was decided that removing both of these variables from the model was necessary.  This is shown in our final model (5) of Table 3.

As a quick check we looked into the VIF values for the predictors and determined that we weren't worried about collinearity between the predictors.

```{r, echo=FALSE, results='asis'}
# VIF Table
out <- capture.output(stargazer(vif(lm.price.final),
          title="VIF for Final Price Model",
          header=FALSE,
          align=FALSE))

out <- sub(" \\\\centering", "", out)
cat(out)
```

```{r, echo=FALSE, fig.align="center", fig.height=2, fig.width=8, fig.cap="Diagnostic Plots of Final Model"}
# Student Residuals and Hat Values Computed
par(mfrow = c(1,4))
plot(lm.price.final,c(1,2,4))
stures <- rstudent(lm.price.final)
plot(stures, main="Student Residuals")
abline(h=2, lty=2)
abline(h=-2, lty=2)
```

Lastly, in an effort to address the values that showed large Cook’s Distance in the diagnostic plots, we compared which leverage points in our model also had large residuals. We found observations that satisfied both these criteria, and concluded that these points needed to be addressed with the client. 


```{r, echo=FALSE, results='asis'}
# Calculate intersection of student residuals with high leverage
hv <- as.matrix(hatvalues(lm.price.final))
mn <- mean(hatvalues(lm.price.final))
stures_vals <- which(abs(stures) > 2)
lev_vals <- which(hv > 2*mn)
index_values <- intersect(lev_vals,stures_vals)
df.new <- df %>% dplyr::select(price,
                        sqft_living,
                        view,
                        grade,
                        yr_built)
stargazer(df.new[index_values,],
          header=FALSE,
          title="High Student Residual and Leverage Values",
          summary=FALSE)
```

\newpage

# Results and Conclusions

Our final fitted model is given by the following

$$
\log(\mathrm{price}) = 16.98 + 0.43 \log(\mathrm{sqftliving}) + 0.24\mathrm{grade} + 0.12\mathrm{view} - 0.0045\mathrm{yr\_built}
$$

*log(sqft_living):*
Holding all else constant, a one percent increase in the living square footage of a home would yield a 0.426% increase in the average price of the home.
This can be more easily understood as a 10 percent increase in the living square footage of a home would yield a 4.26% increase in the average price of the home.

*grade*
Holding all else constant, a one level increase in grade will result in an increase in average price of 26.67 percent.

*view*
Holding all else constant, a one level increase in view will result in an increase in average price of 12.23 percent.

*yr_built*
Holding all else constant, a one year increase in the year the house was built will result in a -.45% decrease in average housing price.

Using this model to predict housing price would be beneficial for a stakeholder with the understanding of some key aspects of the model. We will first start with the year the house was built. In our model we have determined that if all else in the model was held constant, the price of a house built closer to present day, then say 5 years beforehand, would produce a lower estimation of price. Lets look at an example of this. Using a house with a living square footage of 2000, housing unit grade of 10, and have been viewed 1 time by a potential buyer, we would see a price of $\$ 839,675.10$ if it was built in 1990. Holding living square footage, grade, and view constant, and we adjust that house to be built in 2000, we would then estimate an average price of $\$ 802,326.9$, result in a decrease in housing price of $\$37,348.24$.

\newpage

# Appendix: All Code for This Report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```