---
title: "Bryce"
author: "Bryce Smith"
date: "2/24/2021"
output: word_document
---

Notes:
- Categorical Variables: waterfront (0-1), view (0-1), condition (1-5), grade (5-12)
- Quantitative Variables: bedrooms, bathrooms, sqft. sqftlot, floors, sqftabove, sqftbasement, *yrbuilt*, *yrrenovated*
- Exclude: ID
- sqft_living highest correlation with price




*{variable}* = unsure
```{r}
df_mod <- read.csv('Seattle.csv')
attach(df_mod)
summary(df)
head(df,4)
```
#Changing the date value
```{r}
library(lubridate)
df$date <- ymd(substr(date,1,nchar(date) - 7))
head(df,4)
```
#Scatterplots
```{r}
par(mfrow = c(1,2))
plot(df$date, df$price, xlab = "Date", ylab = "Price")
plot(df$bedrooms, df$price, xlab = "bedrooms", ylab = "price")
```
```{r}
par(mfrow = c(1,2))
plot(df$bathrooms, df$price, xlab = "bathrooms", ylab = "price")
plot(df$sqft_living, df$price, xlab = "sqft", ylab = "price")
```
```{r}
par(mfrow = c(1,2))
plot(df$sqft_lot, df$price, xlab = "Lot Square Footage", ylab = "Price")
plot(df$floors, df$price, xlab = "# of Floors", ylab = "Price")
```
```{r}
par(mfrow = c(1,2))
plot(df$condition, df$price, xlab = "Condition", ylab = "Price")
plot(df$grade, df$price, xlab = "Grade", ylab = "Price")
```

```{r}
par(mfrow = c(1,2))
plot(df$sqft_above, df$price, xlab = "sqft_above", ylab = "Price")
plot(df$sqft_basement, df$price, xlab = "sqft_basement", ylab = "Price")
```

```{r}
par(mfrow = c(1,2))
plot(df$yr_built, df$price, xlab = "Year Built", ylab = "Price")
plot(df$yr_renovated, df$price, xlab = "Year Renovated", ylab = "Price")
```
#Correlations
```{r}
cor(df[,c(3,4,5,6,7,8,13)])
```

```{r}
df$bedrooms <- as.factor(df$bedrooms)

ggplot(df, aes(x=bedrooms, y=price, fill=bedrooms)) +
  theme_solarized(light = TRUE) +
  scale_colour_solarized("red") +
  geom_violin(aes(color=bedrooms)) +
  geom_boxplot(width=0.1) +
  ggtitle("Price by Number of Bedrooms") +
  xlab("Number of Bedrooms") +
  ylab("Price") +
  theme(legend.position="none", plot.title = element_text(hjust = 0.5))

```
```{r}
df$floors <- as.factor(df$floors)

ggplot(df, aes(x=floors, y=price, fill=floors)) +
  theme_solarized(light = TRUE) +
  scale_colour_solarized("red") +
  geom_violin(aes(color=floors)) +
  geom_boxplot(width=0.1) +
  ggtitle("Price by Number of Floors") +
  xlab("Number of Floors") +
  ylab("Price") +
  theme(legend.position="none", plot.title = element_text(hjust = 0.5))

```

#Binning bathrooms
```{r}
library(tidyverse)
library(dplyr)
library(scales)
tags <- c("[0-1]", "[1-2]","[2-3]", "[3-4]", "[4-6]")
v <- df %>% select(price, bathrooms)
vgroup <- as_tibble(v) %>% mutate(tag = case_when(
  bathrooms == 0.25|bathrooms == 0.5|bathrooms == 0.75|bathrooms == 1.00 ~ tags[1],
  bathrooms == 1.25|bathrooms == 1.5|bathrooms == 1.75|bathrooms == 2.00 ~ tags[2],
  bathrooms == 2.25|bathrooms == 2.5|bathrooms == 2.75|bathrooms == 3.00 ~ tags[3],
  bathrooms == 3.25|bathrooms == 3.5|bathrooms == 3.75|bathrooms == 4.00 ~ tags[4],
  bathrooms > 4.00 & bathrooms <= 6.00 ~ tags[5],
))
summary(vgroup)
```


#Bins
```{r}
vgroup$tag <- factor(vgroup$tag, levels = tags, ordered = FALSE)
summary(vgroup$tag)
```


#Jitterplots with overlying boxplot
```{r}
ggplot(data = vgroup, mapping = aes(x=tag,y=price)) + 
  geom_jitter(aes(color='blue'),alpha=0.2) +
  geom_boxplot(fill="bisque",color="black",alpha=0.2) + 
  labs(x='Number of Bathrooms', y= 'Housing Price') +
  guides(color=FALSE) +
  theme_minimal() + scale_y_continuous(breaks = c(300000,600000,900000,2000000,4000000,5000000), labels = comma)
```

```{r}
ggplot(data = df, mapping = aes(x=factor(floors),y=price)) + 
  geom_jitter(aes(color='blue'),alpha=0.2) +
  geom_boxplot(fill="bisque",color="black",alpha=0.2) + 
  labs(x='Number of Floors', y= 'Housing Price') +
  guides(color=FALSE)  + scale_y_continuous(breaks = c(300000,600000,900000,2000000,4000000,5000000), labels = comma) + theme_minimal()


```

```{r}
ggplot(data = df, mapping = aes(x=factor(bedrooms),y=price)) + 
  geom_jitter(aes(color='blue'),alpha=0.2) +
  geom_boxplot(fill="bisque",color="black",alpha=0.2) + 
  labs(x='Number of Bedrooms', y= 'Housing Price') +
  guides(color=FALSE)  + scale_y_continuous(breaks = c(300000,600000,900000,2000000,4000000,5000000), labels = comma) + theme_minimal()

```

#Visualizing observations with a basement vs. no basement
```{r}
df$basement <- ifelse(df$sqft_basement == 0, 'No Basement','Basement')
ggplot(df, aes(x=basement, y=price, fill=basement)) +
  theme_solarized(light = TRUE) +
  scale_colour_solarized("red") +
  geom_violin(aes(color=basement)) +
  geom_boxplot(width=0.1) +
  ggtitle("Basement versus No Basement") +
  ylab("Price") +
  theme(legend.position="none", plot.title = element_text(hjust = 0.5))

```

```{r}
library(GGally)
quant.columns <- c(3,6,7,13,14)
ggpairs(df[quant.columns],title = "Correlogram of Quant. Columns")

```

```{r}
cor(bedrooms, sqft_living)
```

```{r}
lm_simp <- lm(price ~ sqft_living, data = df)
summary(lm_simp)
```

```{r}
lm_2 <- lm(price ~ sqft_living + basement, data = df)
summary(lm_2)
lm_6 <- lm(price ~ sqft_living + bedrooms + condition, data = df)
summary(lm_6)
```
```{r}
df$grade <- as.numeric(df$grade)
lm_top <- lm(price ~ sqft_living + waterfront + grade + bedrooms, data = df)
summary(lm_top)
anova(lm_top)
```

```{r}
library(ggplot2)
ggplot(data = df, aes( sqft_living,log(price))) + geom_point(color = factor(df$bathrooms)) + theme()
ggplot(data = df, aes( sqft_living,log(price))) + geom_point(color = factor(df$condition)) + theme()

```

###Model Selection - March 9th###

```{r}
library(lubridate)
library(GGally)
df_mod5 <- read.csv('Seattle.csv')
df_mod5$date <- ymd(substr(df_mod5$date,1,nchar(df_mod5$date) - 7))
df_mod5$basement <- ifelse(df_mod5$sqft_basement == 0, 0,1)
df_mod5$renovated <- ifelse(df_mod5$yr_renovated > 0, 1, 0)
head(df_mod5,3)
```

#Taking a look at distribution, correlation, and scatterplots of the variables
#Excluding categorical indicator variables and first two columns of data
#Placing price at the end so we can see all correlations and all scatterplots

```{r, fig.height = 6, fig.width = 6}
columns_for_plot <- c(4:8,10:16,3)
ggpairs(df_mod5[columns_for_plot],title = "Correlogram of Variables")
```
#Scatterplots: Looks like direct relationship with sqft_above, grade, sqft_living, bathrooms, bedrooms
#Correlations: Top correlation in decending order - sqft_living, grade, sqft_above, bathrooms, view, sqft_basement, bedrooms, etc.

#Going to proceed with adding variables one at a time based upon correlation with price. 

```{r}
mod1 <- lm(price ~ sqft_living, data = df_mod5)
summary(mod1)
```

$R^2 = .5221$
sqft_living is helpful in the predicting price.

#Adding grade to the model
```{r}
mod2 <- lm(price ~ sqft_living + grade, data = df_mod5)
summary(mod2)
```

Higher R^2 adjusted, both grade and sqft_living are significant.

#Going to skip placing sqft_above in the model because it is essentially the same variable as sqft_living

#Adding bathrooms

```{r}
mod3 <- lm(price ~ sqft_living + grade + bathrooms, data = df_mod5)
summary(mod3)
```
#Bathrooms is not significant in this model --> variables added last t-test
#Adding view to the model
```{r}
mod4 <- lm(price ~ sqft_living + grade + view, data = df_mod5)
summary(mod4)
```
#Significant
#Skipping sqft_basement because not all houses have basements
#Will try bedrooms
```{r}
mod5 <- lm(price ~ sqft_living + grade + view + bedrooms, data = df_mod5)
summary(mod5)
```
#Bedrooms not significant. R^2 barely increased. Going to remove. 

#Going to try to add in indicator variables starting with waterfront
```{r}
mod61 <- lm(price ~ sqft_living + grade + view + yr_built, data = df_mod5)
summary(mod61)
```
```{r}
mod6 <- lm(price ~ sqft_living + grade + view + waterfront, data = df_mod5)
summary(mod6)
```
#Significant to model. Good increase in R^2 Adj.
```{r}
mod100 <- lm(price ~ sqft_living + grade + view + renovated, data = df_mod5)
summary(mod100)
```

```{r}
mod101 <- lm(price ~ sqft_living + grade + view + waterfront + basement, data = df_mod5)
summary(mod101)
```

#Lets try yr_built out of curiousity (No strong reason I can find)

```{r}
mod7 <- lm(price ~ sqft_living + grade + view+ yr_built + waterfront , data = df_mod5)
summary(mod7)
```

#Significant and strong increase in R^2 adjusted

#Notice both sqft_living and grade have a strong correlation. Lets try an interaction in the model
```{r}
mod8 <- lm(price ~ sqft_living + grade + view + waterfront + yr_built + sqft_living*grade, data = df_mod5)
summary(mod8)
```
#Interaction is helpful to the model. R^2 adjusted increased to ~.71

#Lets check residual plots

```{r}
par(mfrow = c(1,3))
plot(mod8,c(1,2,4))
```
QQplot is telling us that the residuals are not conforming to a sample from a normal distribution. Looks like very heavy tailed data. Conclude we are not safe assuming that the errors are normally distributed.

Looks like non constant variance, mean not zero, and values with > 2 cooks distance.



#Durbin watson test

```{r}
library(car)
durbinWatsonTest(mod8)
```

Looks like there may not be serial correlation.


```{r}
plot(mod8$fitted.values, mod8$residuals)
abline(a = 0, b = 1)
```
Non constant variance?

#Notice the right skewed distribution for both price and sqft_living. Possible a logarithm transform for these two variables will help fix our assumptions.

```{r}
par(mfrow = (c(2,2)))
plot(density(df_mod5$price))
plot(density(df_mod5$sqft_living))
plot(density(log(df_mod5$price)))
plot(density(log(df_mod5$sqft_living)))
```
The logarithm transformations looks like it normalizes the data pretty well.

#applying it in model

```{r}
mod10 <- lm(log(price) ~ log(sqft_living) + grade + view + waterfront + yr_built + log(sqft_living)*grade, data = df_mod5)
summary(mod10)
```
#Drop in R^2 adjusted. Interaction bordline significant. Waterfront no longer significant

#Remove waterfront
```{r}
mod10 <- lm(log(price) ~ log(sqft_living) + grade + view + yr_built + log(sqft_living)*grade, data = df_mod5)
summary(mod10)
```
#Interaction still borderline significant. Lets try removing it.

```{r}
mod11 <- lm(log(price) ~ log(sqft_living) + grade + view + yr_built, data = df_mod5)
summary(mod11)
anova(mod11)
```

#Interaction removed. All variables significant. R^2 adjusted didnt move.

#Residual plots

```{r}
par(mfrow = c(1,3))
plot(mod11,c(1,2,4))
```
#Linearity looks good.
#Cook's distance still there. 
#Looks like random scatter in residuals vs. fitted.
#Lets look at what our leverage values are
```{r}
hv <- as.matrix(hatvalues(mod11))
mn <- mean(hatvalues(mod11))
as.matrix(hv[hv > 2*mn,])
length(hv[hv > 2*mn,])
```

#53 leverage points..

#Good or bad?
#Student Residuals
```{r}
stures <- rstudent(mod11)
plot(stures)
abline(h=2, lty=2)
abline(h=-2, lty=2)
```
```{r}
as.matrix(stures[abs(stures) > 2])
length(stures[abs(stures) > 2])
```
#24 outliers. Are they leverage values?
```{r}
stures_vals <- which(abs(stures) > 2)
lev_vals <- which(hv > 2*mn)
intersect(lev_vals,stures_vals)
```
#Couldnt figure out code but 40, 47, 244, 346, 432, and 520 are both leverage values and outliers.
#244 and 40 both have a large cook's distance as well.


#Consider collinearity
```{r}
vif(mod11)
```
#Looks good.

#Lastly, residuals vs. predictors

```{r}
par(mfrow = c(2,2))
plot(log(sqft_living), mod11$residuals)
plot(grade, mod11$residuals)
plot(view, mod11$residuals)
plot(yr_built, mod11$residuals)
```
#I'd say it looks like random scatter.

###Analysis Section
To determine the best model to predict price using the ‘Seattle.csv’ dataset was to analyze the correlations of the predictors variables with our response, and to sequentially add predictors to the model while assessing model performance. This analysis began with analyzing a correlogram of the dataset, figure __, excluding id, date, waterfront, renovated and basement. ID and date would not be helpful in our model because id does not give any information on the price of a house, and date would not provide us the type of information we need to predict future houses. The indicator variables waterfront, renovated, and basement were not needed in this visual analysis and will be introduced in the analysis later. 

#Correllogram

Using the correlogram it was clear that the variables sqft_above, grade, sqft_living, bathrooms, and bedrooms had a direct relationship with our response variable price. This proved to be the case as we considered the correlations of the variables in descending order. Using the correlations of these predictors, we began to build our model by adding one variable at a time, and using the variables added last tests to determine significance of the variable in our model, beginning the variable that has the highest correlation with price, sqft_living. As seen in model 1 of table __, it was clear that we had strong evidence for a linear association between price and sqft_living and that we explained roughly 52% of the variation in price.

We then proceeded to add grade to the model. Adding grade to the model proved to be helpful, as we noticed a significant p-value for the variables added last test which was 0.000251. After adding grade to the model, we did not attempt to add ‘sqft_above’ into the model since we determined this variable to be almost identical to ‘sqft_living’, with a correlation value of 0.898 to backup this assumption.

Next we added bathrooms into our model and received a p-value of 0.449107 for the variables added last t-test so it was safe to conclude that it was not helpful to our model with sqft_living and grade already included. However, our next variable we added to our model, view, did prove to be helpful with a p-value of $2e10^{16}$.

With three variables determine to explained roughly 59% of the variation in price, we looked to try to add bedrooms into the model. It was clear with the results from the summary table that bedrooms hardly explained any more variation in price, and was not helpful in our model so we removed that predictor.

At this point in our analysis we concluded to stop adding quantitative variables based off of their correlation to price since we wanted to conserve the parsimonious nature of the model. We did want to try the variable yr_built in the model since we believed it was important to attempt to add a variable that represented the age of the house. Adding yr_built into our model proved to be significant with sqft_living, grade and view already present in the model.

Next we looked to add indicator variables in our model and see if they would be helpful in explaining the variability in price. Starting with waterfront, we found that this indicator was helpful in our model and increased the amount of variation explained in price to roughly 0.65. This can be found in model 2 in table __.

```{r}
mod71 <- lm(price ~ sqft_living + grade + view + yr_built + waterfront, data = df_mod5)
summary(mod71)
```

Adding basement and renovated as indicators to our model, in the same fashion as we added waterfront to the model, ended up not being helpful to our model with sqft_living, grade, view, and yr_built already present. We also added these two indicators, separately, with waterfront in the model, and the were not helpful either.

At this point we are satisfied with the four predictors, sqft_living, grade, view and waterfront being used in our model.

Before moving on in our analysis, we wanted to ensure that our model doesn't disagree with an alternative analysis built from a full model in that we did not miss any potential predictors that may add to our model's explained variation while offering strong evidence that they linearly associated with the predictor.  To accomplish this, we first compared a model with nearly every predictor with the exception of those predictors that could be eliminated due to their intrinsic nature or prior inspection. For instance, the id field or date of transaction was not considered in the full model.  The following anova table indicated that their may be some predictors which may be linearly associated with the response variable.

We then removed the predictors from the nearly full model by examining those with the weakest p-value evidence, and ended up with a reduced model which discarded floors, sqft_above, and sqft_lot.  Comparing the adjusted R^2 value of this reduced model and the R^2 model of the one which we arrived at in our model built up from initial predictors indicates that the additional explained variation is minimal.  Seeking a simpler model, we feel confident that our model will perform accurately after residual error and examination of the diagnostic plots.

Proceeding with model 2 in our analysis, we needed to assess the correlations between our predictors. When doing this, it was clear that we had evidence of collinearity between sqft_living and grade. With this knowledge we sought to assess whether an interaction between these two variables would be helpful to our model. As seen in model 3 of table __, adding this interaction term proved to be helpful in our model and increased the variation in priced explained by the model to 0.706.

Satisfied with this model, we proceeded to check the diagnostic plots to check the validity of our assumptions. After review of these plots, it was clear we had problems addressing our assumptions of linearity, mean zero for the errors and constant variance of the errors. This can be shown with the fanning out of the points in the residuals vs fitted plot, and the lack of linearity in the normal QQ plot. We can also see that we have observations with very large Cook’s Distance, which may be negatively affecting our model. Checking the residuals versus the predicted values confirmed what we saw in the previous three plots and we concluded that our assumptions were violated.

```{r}
par(mfrow = c(1,3))
plot(mod8,c(1,2,4))
plot(mod8$fitted.values, mod8$residuals)
abline(a = 0, b = 1)
```

To address the violations of our assumptions, we entertained the idea of transforming some of the variables used in the model. We could see from the correlogram that both our response, price, and predictor, sqft_living, had heavily right skewed distributions. We decided to apply a logarithmic transformation on both of these variables and reapply them into our model. Doing so we saw our interaction between the now logarithmic transformation of sqft_living and grade become not helpful to our model. We also, saw that the indicator, waterfront, was no longer helpful to the model. This can be seen in model 4 of table __. In an effort to satisfy our assumptions of the model, it was decided that removing both of these variables from the model was necessary.


With model 5 in table __ showing evidence that all variables were helpful in our model, we moved to assess the diagnostic plots. Based off our diagnostic plots we concluded that our assumptions of linearity of the model, errors with mean of zero and constant variance in our residuals were satisfied. We proceeded to conclude this as our final model in predicting price.

```{r}
par(mfrow = c(1,3))
plot(mod11,c(1,2,4))
```

As a quick check we looked into the VIF values for the predictors and determined that we weren't worried about collinearity between the predictors.

```{r}
vif(mod11)
```


Lastly, in an effort to address the values that showed large Cook’s Distance in the diagnostic plots, we compared which leverage points in our model also had large residuals. We found observations that satisfied both these criteria, and concluded that these points needed to be addressed with the client. 

```{r}
stures <- rstudent(mod11)
plot(stures)
abline(h=2, lty=2)
abline(h=-2, lty=2)
stures_vals <- which(abs(stures) > 2)
lev_vals <- which(hv > 2*mn)
intersect(lev_vals,stures_vals)
```


###Results and Conclusions


##Final Model##


##Fitted Model##
log(price) = 16.98 + 0.43(log(sqftliving)) + 0.24(grade) + 0.12(view) - 0.0045(yr_built)

##Summary Table##
```{r}
summary(mod11)
```


##ANOVA Table##
```{r}
anova(mod11)
```

##Interpretation##
*log(sqft_living):*
Holding all else constant, a one percent increase in the living square footage of a home would yield a 0.426% increase in the average price of the home.
This can be more easily understood as a 10 percent increase in the living square footage of a home would yield a 4.26% increase in the average price of the home.

*grade*
Holding all else constant, a one level increase in grade will result in an increase in average price of 26.67 percent.

*view*
Holding all else constant, a one level increase in view will result in an increase in average price of 12.23 percent.

*yr_built*
Holding all else constant, a one year increase in the year the house was built will result in a -.45% decrease in average housing price.

##Discussion##
Using this model to predict housing price would be beneficial for a stakeholder with the understanding of some key aspects of the model. We will first start with the year the house was built. In our model we have determined that if all else in the model was held constant, the price of a house built closer to present day, then say 5 years beforehand, would produce a lower estimation of price. Lets look at an example of this. Using a house with a living square footage of 2000, housing unit grade of 10, and have been viewed 1 time by a potential buyer, we would see a price of $\$ 839,675.10$ if it was built in 1990. Holding living square footage, grade, and view constant, and we adjust that house to be built in 2000, we would then estimate an average price of $\$ 802,326.9$, result in a decrease in housing price of $\$37,348.24$.

```{r}
est1 <- exp(predict(mod11, data.frame(sqft_living = 2000,
                          grade = 10,
                          view = 1,
                          yr_built = 1990)))
est2 <- exp(predict(mod11, data.frame(sqft_living = 2000,
                          grade = 10,
                          view = 1,
                          yr_built = 2000)))
est1
est2
est2 - est1
```

```{r}
plot(df_mod5$yr_built, df_mod5$price)
check <- lm(price ~ yr_built, data = df_mod5)
coefficients(check)
abline(check)
```



